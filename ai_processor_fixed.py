# ai_processor_fixed.py
import logging
import re
from typing import List, Dict
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class AIProcessorFixed:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π InferenceClient
        self.client = InferenceClient(
            token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
    def summarize_text(self, text: str, max_length: int = 150) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Hugging Face API"""
        try:
            clean_text = self.clean_text(text)
            
            # –î–ª—è –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(clean_text.split()) < 15:
                return clean_text
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            summary = self.client.summarization(
                clean_text,
                max_length=max_length,
                min_length=30,
                do_sample=False
            )
            
            if hasattr(summary, 'summary_text'):
                return summary.summary_text
            else:
                # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
                return self.simple_summarize(clean_text)
                
        except Exception as e:
            logger.error(f"Summarization error: {e}")
            return self.simple_summarize(text)
    
    def analyze_sentiment(self, text: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ sentiment
            result = self.client.text_classification(
                text[:512],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                model="cardiffnlp/twitter-roberta-base-sentiment-latest"
            )
            
            if result and len(result) > 0:
                # –ë–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                top_label = result[0]
                label_map = {
                    'LABEL_0': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
                    'LABEL_1': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', 
                    'LABEL_2': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
                }
                return label_map.get(top_label['label'], '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π')
                
        except Exception as e:
            logger.error(f"Sentiment analysis error: {e}")
        
        # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑
        return self.simple_sentiment(text)
    
    def simple_summarize(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(sentences) <= 2:
            return text
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if len(sentences) >= 3:
            return f"{sentences[0]}. {sentences[-1]}."
        else:
            return sentences[0] + '.'
    
    def simple_sentiment(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        positive_words = ['–æ—Ç–ª–∏—á–Ω', '—Ö–æ—Ä–æ—à', '—É—Å–ø–µ—Ö', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '—Ä–µ–∫–æ–º–µ–Ω–¥', '–¥–æ–≤–æ–ª–µ–Ω', '—Å—É–ø–µ—Ä']
        negative_words = ['–ø—Ä–æ–±–ª–µ–º', '–æ—à–∏–±–∫', '–ø–ª–æ—Ö', '—Å–ª–æ–∂–Ω', '–Ω–µ—É–¥–æ–±–Ω', '—É–∂–∞—Å–Ω']
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        elif neg_count > pos_count:
            return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        else:
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π'
    
    def extract_topic(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º—É —Ç–µ–∫—Å—Ç–∞"""
        topics = {
            '–∞–∫—Ü–∏–∏': ['—Å–∫–∏–¥–∫', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂', '–∞–∫—Ü–∏', '–±–æ–Ω—É—Å', '–ø—Ä–æ–º–æ–∫–æ–¥'],
            '–Ω–æ–≤–æ—Å—Ç–∏': ['–∑–∞–ø—É—Å–∫', '–æ–±–Ω–æ–≤–ª–µ–Ω', '–Ω–æ–≤—ã–π', '–∞–Ω–æ–Ω—Å', '—Ä–µ–ª–∏–∑'],
            '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ': ['–ø–∞—Ä—Ç–Ω–µ—Ä', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è'],
            '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫', '–æ—Ç–ø—Ä–∞–≤–∫', '–ø–æ–ª—É—á–µ–Ω', '–∫—É—Ä—å–µ—Ä']
        }
        
        text_lower = text.lower()
        for topic, keywords in topics.items():
            if any(keyword in text_lower for keyword in keywords):
                return topic
        
        return '–Ω–æ–≤–æ—Å—Ç–∏'
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'[@#]\w+', '', text)
        text = re.sub(r'[^\w\s\.\!\?,:;-]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def structure_content(self, main_posts: List[str], discussion_posts: List[str]) -> Dict:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é AI"""
        logger.info("üß† AI —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        try:
            main_summaries = []
            main_topics = set()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ—Å—Ç—ã
            for post in main_posts[:3]:
                if len(post.strip()) > 20:
                    summary = self.summarize_text(post)
                    topic = self.extract_topic(post)
                    main_summaries.append(summary)
                    main_topics.add(topic)
                    logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Å—Ç: {topic}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏—Å–∫—É—Å—Å–∏–æ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –ø–æ —Ç–µ–º–µ
            discussion_insights = []
            for post in discussion_posts:
                if len(post.strip()) > 20:
                    post_topic = self.extract_topic(post)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã
                    if any(topic in post_topic for topic in main_topics) or not main_topics:
                        sentiment = self.analyze_sentiment(post)
                        summary = self.summarize_text(post)
                        discussion_insights.append({
                            'text': summary,
                            'sentiment': sentiment
                        })
                        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –∏–Ω—Å–∞–π—Ç: {sentiment}")
            
            return {
                'main_topic': ' | '.join(list(main_topics)[:2]) if main_topics else '–Ω–æ–≤–æ—Å—Ç–∏',
                'main_content': main_summaries,
                'discussion_insights': discussion_insights[:3],
                'has_ai_analysis': len(main_summaries) > 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return {
                'main_topic': '–Ω–æ–≤–æ—Å—Ç–∏',
                'main_content': main_posts[:2],
                'discussion_insights': [],
                'has_ai_analysis': False
            }

# –¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
def test_fixed_processor():
    processor = AIProcessorFixed()
    
    test_posts = [
        "Ozon –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤. –¢–µ–ø–µ—Ä—å —É—á–∞—Å—Ç–Ω–∏–∫–∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞ —Å–º–æ–≥—É—Ç –ø–æ–ª—É—á–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ –æ–±—ä–µ–º –ø—Ä–æ–¥–∞–∂ –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã. –ü—Ä–æ–≥—Ä–∞–º–º–∞ –Ω–∞—á–Ω–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–µ—Å—è—Ü–∞.",
        "–°–∫–∏–¥–∫–∏ –¥–æ 50% –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫—É –≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ! –£—Å–ø–µ–π—Ç–µ –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏ —Å–º–∞—Ä—Ç—Ñ–æ–Ω—ã, –Ω–æ—É—Ç–±—É–∫–∏ –∏ –ø–ª–∞–Ω—à–µ—Ç—ã –ø–æ –≤—ã–≥–æ–¥–Ω—ã–º —Ü–µ–Ω–∞–º. –ê–∫—Ü–∏—è –¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞.",
        "Wildberries —É–ª—É—á—à–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É –¥–æ—Å—Ç–∞–≤–∫–∏: —Ç–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–æ–≤ –æ—Ç 1000 —Ä—É–±–ª–µ–π. –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –ø—É–Ω–∫—Ç—ã –≤—ã–¥–∞—á–∏ –≤ –ú–æ—Å–∫–≤–µ –∏ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ."
    ]
    
    discussion_posts = [
        "–û—Ç–ª–∏—á–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å –ø—Ä–æ Ozon! –Ø –∫–∞–∫ –ø—Ä–æ–¥–∞–≤–µ—Ü –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –Ω–æ–≤—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏.",
        "–°–∫–∏–¥–∫–∏ –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫—É –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä! –£–∂–µ –ø—Ä–∏—Å–º–æ—Ç—Ä–µ–ª —Å–µ–±–µ –Ω–æ–≤—ã–π –Ω–æ—É—Ç–±—É–∫ –ø–æ –æ—Ç–ª–∏—á–Ω–æ–π —Ü–µ–Ω–µ.",
        "–ë—ã–ª–∏ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –Ω–∞ Wildberries, –Ω–æ –≤ —Ü–µ–ª–æ–º —Å–µ—Ä–≤–∏—Å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª—É—á—à–µ."
    ]
    
    print("üß† –¢–ï–°–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ì–û AI –ü–†–û–¶–ï–°–°–û–†–ê")
    print("=" * 50)
    
    # –¢–µ—Å—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    for i, post in enumerate(test_posts, 1):
        summary = processor.summarize_text(post)
        sentiment = processor.analyze_sentiment(post)
        topic = processor.extract_topic(post)
        
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä {i} ({topic}, {sentiment}):")
        print(f"–û—Ä–∏–≥–∏–Ω–∞–ª: {post[:100]}...")
        print(f"üìã AI-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {summary}")
    
    # –¢–µ—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
    print(f"\nüéØ –¢–ï–°–¢ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ò–Ø:")
    structured = processor.structure_content(test_posts, discussion_posts)
    
    print(f"–¢–µ–º–∞: {structured['main_topic']}")
    print(f"–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç: {len(structured['main_content'])} –ø–æ—Å—Ç–æ–≤")
    print(f"–ò–Ω—Å–∞–π—Ç—ã: {len(structured['discussion_insights'])} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤")
    
    for insight in structured['discussion_insights']:
        print(f"  - {insight['sentiment']}: {insight['text']}")

if __name__ == "__main__":
    test_fixed_processor()