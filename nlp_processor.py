from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from config import SUMMARY_MODEL, COMMENT_MODEL
import re

class NLPProcessor:
    def __init__(self):
        self.summarizer = None
        self.comment_analyzer = None
        self.tokenizer = None
        
    def load_models(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç NLP –º–æ–¥–µ–ª–∏"""
        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            self.summarizer = pipeline(
                "summarization",
                model=SUMMARY_MODEL,
                tokenizer=SUMMARY_MODEL,
                device=-1  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
            )
            
            # –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            self.comment_analyzer = pipeline(
                "text-classification",
                model=COMMENT_MODEL,
                device=-1
            )
            
            print("NLP –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
    
    def summarize_text(self, text, max_length=150, min_length=30):
        """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        if not self.summarizer:
            self.load_models()
            
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if len(text) > 1024:
                text = text[:1024]
                
            summary = self.summarizer(
                text,
                max_length=max_length,
                min_length=min_length,
                do_sample=False
            )
            
            return summary[0]['summary_text']
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return text[:200] + "..."  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    
    def analyze_discussion_tone(self, texts):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ–±—Å—É–∂–¥–µ–Ω–∏–π"""
        if not self.comment_analyzer:
            self.load_models()
            
        try:
            sentiments = []
            for text in texts[:10]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ç–µ–∫—Å—Ç–æ–≤
                if len(text) > 512:
                    text = text[:512]
                    
                result = self.comment_analyzer(text)
                sentiments.append(result[0])
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
            positive_count = sum(1 for s in sentiments if s['label'] == 'LABEL_1')
            negative_count = sum(1 for s in sentiments if s['label'] == 'LABEL_0')
            
            if positive_count > negative_count:
                return "–í –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ"
            elif negative_count > positive_count:
                return "–í –æ—Å–Ω–æ–≤–Ω–æ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ"
            else:
                return "–°–º–µ—à–∞–Ω–Ω—ã–µ"
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å"
    
    def process_posts(self, main_posts, discussion_posts):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å—Ç—ã –∏ —Å–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        if not main_posts:
            return "–ù–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"
        
        # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ—Å—Ç—ã
        summarized_content = []
        for post in main_posts[:5]:  # –ë–µ—Ä–µ–º 5 —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö –ø–æ—Å—Ç–æ–≤
            summary = self.summarize_text(post.text)
            summarized_content.append(f"‚Ä¢ {summary}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏—è
        discussion_texts = [post.text for post in discussion_posts]
        tone = self.analyze_discussion_tone(discussion_texts)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
        final_post = "üìä **–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π**\n\n"
        final_post += "**–û—Å–Ω–æ–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è:**\n"
        final_post += "\n".join(summarized_content)
        final_post += f"\n\nüí≠ **–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–Ω–µ–Ω–∏–µ:** {tone}\n"
        final_post += f"\nüìÖ –ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {len(main_posts)} –¥–Ω–µ–π\n"
        final_post += "#–æ–±–∑–æ—Ä #–Ω–æ–≤–æ—Å—Ç–∏ #–∞–Ω–∞–ª–∏—Ç–∏–∫–∞"
        
        return final_post