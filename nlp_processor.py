from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from config import SUMMARY_MODEL, COMMENT_MODEL, PROCESSING_BATCH_SIZE
import re
import logging
from typing import List

logger = logging.getLogger(__name__)

class NLPProcessor:
    def __init__(self):
        self.summarizer = None
        self.comment_analyzer = None
        self.models_loaded = False
        
    def load_models(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç NLP –º–æ–¥–µ–ª–∏"""
        if self.models_loaded:
            return
            
        try:
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º NLP –º–æ–¥–µ–ª–∏...")
            
            # –ú–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            self.summarizer = pipeline(
                "summarization",
                model=SUMMARY_MODEL,
                tokenizer=SUMMARY_MODEL,
                device=-1,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU
                torch_dtype=torch.float32
            )
            
            # –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            self.comment_analyzer = pipeline(
                "text-classification",
                model=COMMENT_MODEL,
                device=-1
            )
            
            self.models_loaded = True
            logger.info("NLP –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
            # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
            self.models_loaded = False
    
    def summarize_text(self, text: str, max_length: int = 150, min_length: int = 30) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        if not self.models_loaded:
            self.load_models()
            
        if not self.summarizer:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            return text[:200] + "..." if len(text) > 200 else text
            
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if len(text) > 1024:
                text = text[:1024]
                
            summary = self.summarizer(
                text,
                max_length=max_length,
                min_length=min_length,
                do_sample=False,
                truncation=True
            )
            
            return summary[0]['summary_text']
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return text[:200] + "..."  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    
    def analyze_discussion_tone(self, texts: List[str]) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ–±—Å—É–∂–¥–µ–Ω–∏–π"""
        if not self.models_loaded:
            self.load_models()
            
        if not self.comment_analyzer or not texts:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å"
            
        try:
            sentiments = []
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
            for text in texts[:PROCESSING_BATCH_SIZE]:
                if len(text) > 512:
                    text = text[:512]
                    
                result = self.comment_analyzer(text)
                sentiments.append(result[0])
                
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏)
            if sentiments and 'label' in sentiments[0]:
                # –î–ª—è rubert-tiny2: LABEL_0 - –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π, LABEL_1 - –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π
                positive_count = sum(1 for s in sentiments if s['label'] == 'LABEL_1')
                negative_count = sum(1 for s in sentiments if s['label'] == 'LABEL_0')
                
                total = len(sentiments)
                if total > 0:
                    positive_percent = (positive_count / total) * 100
                    
                    if positive_percent > 60:
                        return "üìà –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ"
                    elif positive_percent > 40:
                        return "üìä –°–º–µ—à–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è"
                    else:
                        return "üìâ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ"
            
            return "ü§î –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∏–ª–∏ —Å–º–µ—à–∞–Ω–Ω—ã–µ"
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å"
    
    def process_posts(self, main_posts, discussion_posts):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å—Ç—ã –∏ —Å–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        if not main_posts:
            return "‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥."
        
        self.load_models()
        
        # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ—Å—Ç—ã
        summarized_content = []
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(main_posts[:3])} –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤...")
        
        for i, post in enumerate(main_posts[:3]):  # –ë–µ—Ä–µ–º 3 —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö –ø–æ—Å—Ç–∞
            logger.info(f"–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –ø–æ—Å—Ç {i+1}...")
            summary = self.summarize_text(post.text)
            summarized_content.append(f"‚Ä¢ {summary}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏—è
        discussion_texts = [post.text for post in discussion_posts]
        tone = self.analyze_discussion_tone(discussion_texts)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç
        final_post = "üìä **–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –æ–±–∑–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π**\n\n"
        final_post += "**üéØ –û—Å–Ω–æ–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è:**\n"
        final_post += "\n".join(summarized_content)
        
        if discussion_posts:
            final_post += f"\n\n**üí≠ –û–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–Ω–µ–Ω–∏–µ:** {tone}\n"
            final_post += f"*–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ {min(len(discussion_posts), PROCESSING_BATCH_SIZE)} –æ–±—Å—É–∂–¥–µ–Ω–∏–π*"
        
        final_post += f"\n\nüìÖ **–ü–µ—Ä–∏–æ–¥ –æ–±–∑–æ—Ä–∞:** –ø–æ—Å–ª–µ–¥–Ω–∏–µ {len(main_posts)} –¥–Ω–µ–π"
        final_post += "\n\n#–æ–±–∑–æ—Ä #–Ω–æ–≤–æ—Å—Ç–∏ #–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ #–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è"
        
        logger.info("–ü–æ—Å—Ç —É—Å–ø–µ—à–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω")
        return final_post