# ai_processor_final.py
import logging
import re
from typing import List, Dict
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class AIProcessorFinal:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π InferenceClient
        self.client = InferenceClient(
            token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
    def summarize_text(self, text: str) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Hugging Face API"""
        try:
            clean_text = self.clean_text(text)
            
            # –î–ª—è –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(clean_text.split()) < 15:
                return clean_text
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            summary = self.client.summarization(
                clean_text,
                parameters={
                    'max_length': 150,
                    'min_length': 30,
                    'do_sample': False
                }
            )
            
            if hasattr(summary, 'summary_text'):
                return summary.summary_text
            else:
                # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
                return self.simple_summarize(clean_text)
                
        except Exception as e:
            logger.error(f"Summarization error: {e}")
            return self.simple_summarize(text)
    
    def analyze_sentiment(self, text: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ sentiment
            result = self.client.text_classification(
                text[:512]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            )  # –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è sentiment analysis
            
            if result and len(result) > 0:
                # –ë–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                top_label = result[0]
                label_map = {
                    'LABEL_0': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
                    'LABEL_1': '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', 
                    'LABEL_2': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π',
                    'NEGATIVE': '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π',
                    'POSITIVE': '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
                }
                return label_map.get(top_label['label'], '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π')
                
        except Exception as e:
            logger.error(f"Sentiment analysis error: {e}")
        
        # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑
        return self.simple_sentiment(text)
    
    def simple_summarize(self, text: str) -> str:
        """–£–º–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(sentences) <= 2:
            return text
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –ª—É—á—à–µ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        key_sentences = []
        
        # 1. –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–æ–±—ã—á–Ω–æ –≤–≤–µ–¥–µ–Ω–∏–µ)
        if sentences:
            key_sentences.append(sentences[0])
        
        # 2. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        key_words = ['–Ω–æ–≤—ã–π', '–∑–∞–ø—É—Å–∫', '—Å–∫–∏–¥–∫', '–∞–∫—Ü–∏', '–≤–∞–∂–Ω–æ', '–∏–∑–º–µ–Ω–µ–Ω', '—É–ª—É—á—à–µ–Ω–∏']
        for sentence in sentences[1:]:
            if any(keyword in sentence.lower() for keyword in key_words):
                if sentence not in key_sentences:
                    key_sentences.append(sentence)
                    if len(key_sentences) >= 2:  # –ú–∞–∫—Å–∏–º—É–º 2 –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                        break
        
        # 3. –ï—Å–ª–∏ –º–∞–ª–æ –∫–ª—é—á–µ–≤—ã—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ
        if len(key_sentences) < 2 and len(sentences) > 1:
            key_sentences.append(sentences[-1])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        result = '. '.join(key_sentences[:3]) + '.'
        if len(result) > 400:
            result = result[:397] + '...'
        
        return result
    
    def simple_sentiment(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        positive_words = ['–æ—Ç–ª–∏—á–Ω', '—Ö–æ—Ä–æ—à', '—É—Å–ø–µ—Ö', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '—Ä–µ–∫–æ–º–µ–Ω–¥', '–¥–æ–≤–æ–ª–µ–Ω', '—Å—É–ø–µ—Ä', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω']
        negative_words = ['–ø—Ä–æ–±–ª–µ–º', '–æ—à–∏–±–∫', '–ø–ª–æ—Ö', '—Å–ª–æ–∂–Ω', '–Ω–µ—É–¥–æ–±–Ω', '—É–∂–∞—Å–Ω', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω']
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        elif neg_count > pos_count:
            return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        else:
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π'
    
    def extract_topic(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º—É —Ç–µ–∫—Å—Ç–∞"""
        topics = {
            '–∞–∫—Ü–∏–∏': ['—Å–∫–∏–¥–∫', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂', '–∞–∫—Ü–∏', '–±–æ–Ω—É—Å', '–ø—Ä–æ–º–æ–∫–æ–¥', '%'],
            '–Ω–æ–≤–æ—Å—Ç–∏': ['–∑–∞–ø—É—Å–∫', '–æ–±–Ω–æ–≤–ª–µ–Ω', '–Ω–æ–≤—ã–π', '–∞–Ω–æ–Ω—Å', '—Ä–µ–ª–∏–∑', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º'],
            '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ': ['–ø–∞—Ä—Ç–Ω–µ—Ä', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', '–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ'],
            '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫', '–æ—Ç–ø—Ä–∞–≤–∫', '–ø–æ–ª—É—á–µ–Ω', '–∫—É—Ä—å–µ—Ä', '–ø—É–Ω–∫—Ç –≤—ã–¥–∞—á–∏'],
            '–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å': ['ozon', 'wildberries', '—è–Ω–¥–µ–∫—Å –º–∞—Ä–∫–µ—Ç', 'marketplace']
        }
        
        text_lower = text.lower()
        for topic, keywords in topics.items():
            if any(keyword in text_lower for keyword in keywords):
                return topic
        
        return '–Ω–æ–≤–æ—Å—Ç–∏'
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'[@#]\w+', '', text)
        text = re.sub(r'[^\w\s\.\!\?,:;-]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def structure_content(self, main_posts: List[str], discussion_posts: List[str]) -> Dict:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é AI"""
        logger.info("üß† AI —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        try:
            main_summaries = []
            main_topics = set()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ—Å—Ç—ã
            for post in main_posts[:3]:
                if len(post.strip()) > 20:
                    summary = self.summarize_text(post)
                    topic = self.extract_topic(post)
                    main_summaries.append(summary)
                    main_topics.add(topic)
                    logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Å—Ç: {topic}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏—Å–∫—É—Å—Å–∏–æ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –ø–æ —Ç–µ–º–µ
            discussion_insights = []
            for post in discussion_posts:
                if len(post.strip()) > 20:
                    post_topic = self.extract_topic(post)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã
                    if any(topic in post_topic for topic in main_topics) or not main_topics:
                        sentiment = self.analyze_sentiment(post)
                        summary = self.summarize_text(post)
                        discussion_insights.append({
                            'text': summary,
                            'sentiment': sentiment
                        })
                        if len(discussion_insights) >= 3:  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–Ω—Å–∞–π—Ç–∞
                            break
            
            return {
                'main_topic': ' | '.join(list(main_topics)[:2]) if main_topics else '–Ω–æ–≤–æ—Å—Ç–∏',
                'main_content': main_summaries,
                'discussion_insights': discussion_insights,
                'has_ai_analysis': len(main_summaries) > 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return {
                'main_topic': '–Ω–æ–≤–æ—Å—Ç–∏',
                'main_content': [self.simple_summarize(post) for post in main_posts[:2]],
                'discussion_insights': [],
                'has_ai_analysis': False
            }

# –¢–µ—Å—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
def test_final_processor():
    processor = AIProcessorFinal()
    
    test_posts = [
        "Ozon –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤. –¢–µ–ø–µ—Ä—å —É—á–∞—Å—Ç–Ω–∏–∫–∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞ —Å–º–æ–≥—É—Ç –ø–æ–ª—É—á–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ –æ–±—ä–µ–º –ø—Ä–æ–¥–∞–∂ –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã. –ü—Ä–æ–≥—Ä–∞–º–º–∞ –Ω–∞—á–Ω–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å–ª–µ–¥—É—é—â–µ–≥–æ –º–µ—Å—è—Ü–∞ –∏ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –ø—Ä–æ–¥–∞–≤—Ü–æ–≤.",
        "–°–∫–∏–¥–∫–∏ –¥–æ 50% –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫—É –≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ! –£—Å–ø–µ–π—Ç–µ –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏ —Å–º–∞—Ä—Ç—Ñ–æ–Ω—ã, –Ω–æ—É—Ç–±—É–∫–∏ –∏ –ø–ª–∞–Ω—à–µ—Ç—ã –ø–æ –≤—ã–≥–æ–¥–Ω—ã–º —Ü–µ–Ω–∞–º. –ê–∫—Ü–∏—è –¥–µ–π—Å—Ç–≤—É–µ—Ç –¥–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞. –¢–æ–ª—å–∫–æ –¥–ª—è –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.",
        "Wildberries —É–ª—É—á—à–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É –¥–æ—Å—Ç–∞–≤–∫–∏: —Ç–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫–∞–∑–æ–≤ –æ—Ç 1000 —Ä—É–±–ª–µ–π. –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –ø—É–Ω–∫—Ç—ã –≤—ã–¥–∞—á–∏ –≤ –ú–æ—Å–∫–≤–µ –∏ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ. –≠—Ç–æ —Å–¥–µ–ª–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –µ—â–µ —É–¥–æ–±–Ω–µ–µ –¥–ª—è –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π."
    ]
    
    discussion_posts = [
        "–û—Ç–ª–∏—á–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å –ø—Ä–æ Ozon! –Ø –∫–∞–∫ –ø—Ä–æ–¥–∞–≤–µ—Ü –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –Ω–æ–≤—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –º–æ—Ç–∏–≤–∏—Ä—É–µ—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ.",
        "–°–∫–∏–¥–∫–∏ –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫—É –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä! –£–∂–µ –ø—Ä–∏—Å–º–æ—Ç—Ä–µ–ª —Å–µ–±–µ –Ω–æ–≤—ã–π –Ω–æ—É—Ç–±—É–∫ –ø–æ –æ—Ç–ª–∏—á–Ω–æ–π —Ü–µ–Ω–µ. –ñ–¥—É –Ω–µ –¥–æ–∂–¥—É—Å—å –Ω–∞—á–∞–ª–∞ –∞–∫—Ü–∏–∏!",
        "–ë—ã–ª–∏ –Ω–µ–±–æ–ª—å—à–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –Ω–∞ Wildberries, –Ω–æ –≤ —Ü–µ–ª–æ–º —Å–µ—Ä–≤–∏—Å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª—É—á—à–µ. –ù–æ–≤—ã–µ –ø—É–Ω–∫—Ç—ã –≤—ã–¥–∞—á–∏ - —ç—Ç–æ —É–¥–æ–±–Ω–æ!",
        "Ozon –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è, —ç—Ç–æ —Ä–∞–¥—É–µ—Ç. –ù–æ–≤—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ –ø–æ–º–æ–≥–∞—é—Ç —Ä–∞—Å—Ç–∏ –≤–º–µ—Å—Ç–µ —Å –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–º."
    ]
    
    print("üß† –§–ò–ù–ê–õ–¨–ù–´–ô AI –ü–†–û–¶–ï–°–°–û–† –° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò")
    print("=" * 60)
    
    # –¢–µ—Å—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    print("\nüìä –¢–ï–°–¢ –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò:")
    for i, post in enumerate(test_posts, 1):
        summary = processor.summarize_text(post)
        sentiment = processor.analyze_sentiment(post)
        topic = processor.extract_topic(post)
        
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä {i} ({topic.upper()}, {sentiment}):")
        print(f"–û—Ä–∏–≥–∏–Ω–∞–ª: {post[:120]}...")
        print(f"üìã AI-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {summary}")
    
    # –¢–µ—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
    print(f"\nüéØ –¢–ï–°–¢ –ü–û–õ–ù–û–ì–û –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ò–Ø:")
    print("–°–æ–±–∏—Ä–∞–µ–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç...")
    
    structured = processor.structure_content(test_posts, discussion_posts)
    
    print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"üè∑Ô∏è  –û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞: {structured['main_topic']}")
    print(f"üìã –û—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤: {len(structured['main_content'])}")
    print(f"üí≠ –ò–Ω—Å–∞–π—Ç–æ–≤: {len(structured['discussion_insights'])}")
    
    print(f"\nüì∞ –û–°–ù–û–í–ù–û–ô –ö–û–ù–¢–ï–ù–¢:")
    for i, content in enumerate(structured['main_content'], 1):
        print(f"  {i}. {content}")
    
    if structured['discussion_insights']:
        print(f"\nüë• –ß–¢–û –î–£–ú–ê–Æ–¢ –î–†–£–ì–ò–ï:")
        for insight in structured['discussion_insights']:
            emoji = "üòä" if insight['sentiment'] == '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π' else "üòê" if insight['sentiment'] == '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π' else "üòü"
            print(f"  {emoji} {insight['text']}")

if __name__ == "__main__":
    test_final_processor()