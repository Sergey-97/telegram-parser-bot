# ai_processor_final.py
import logging
import re
from typing import List, Dict
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class AIProcessorFinal:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π InferenceClient
        self.client = InferenceClient(
            token=os.getenv('HUGGINGFACE_TOKEN')
        )
        
    def summarize_text(self, text: str) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Hugging Face API"""
        try:
            clean_text = self.clean_text(text)
            
            # –î–ª—è –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(clean_text.split()) < 15:
                return clean_text
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º
            summary = self.client.summarization(clean_text)
            
            if hasattr(summary, 'summary_text'):
                return summary.summary_text
            else:
                # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
                return self.simple_summarize(clean_text)
                
        except Exception as e:
            logger.error(f"Summarization error: {e}")
            return self.simple_summarize(text)
    
    def simple_summarize(self, text: str) -> str:
        """–£–º–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(sentences) <= 2:
            return text
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –ª—É—á—à–µ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        key_sentences = []
        
        # 1. –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–æ–±—ã—á–Ω–æ –≤–≤–µ–¥–µ–Ω–∏–µ)
        if sentences:
            key_sentences.append(sentences[0])
        
        # 2. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        key_words = ['–Ω–æ–≤—ã–π', '–∑–∞–ø—É—Å–∫', '—Å–∫–∏–¥–∫', '–∞–∫—Ü–∏', '–≤–∞–∂–Ω–æ', '–∏–∑–º–µ–Ω–µ–Ω', '—É–ª—É—á—à–µ–Ω–∏']
        for sentence in sentences[1:]:
            if any(keyword in sentence.lower() for keyword in key_words):
                if sentence not in key_sentences:
                    key_sentences.append(sentence)
                    if len(key_sentences) >= 2:  # –ú–∞–∫—Å–∏–º—É–º 2 –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                        break
        
        # 3. –ï—Å–ª–∏ –º–∞–ª–æ –∫–ª—é—á–µ–≤—ã—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ
        if len(key_sentences) < 2 and len(sentences) > 1:
            key_sentences.append(sentences[-1])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        result = '. '.join(key_sentences[:3])
        if len(result) > 400:
            result = result[:397] + '...'
        
        return result
    
    def analyze_sentiment(self, text: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        return self.simple_sentiment(text)
    
    def simple_sentiment(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        positive_words = ['–æ—Ç–ª–∏—á–Ω', '—Ö–æ—Ä–æ—à', '—É—Å–ø–µ—Ö', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '—Ä–µ–∫–æ–º–µ–Ω–¥', '–¥–æ–≤–æ–ª–µ–Ω', '—Å—É–ø–µ—Ä', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω']
        negative_words = ['–ø—Ä–æ–±–ª–µ–º', '–æ—à–∏–±–∫', '–ø–ª–æ—Ö', '—Å–ª–æ–∂–Ω', '–Ω–µ—É–¥–æ–±–Ω', '—É–∂–∞—Å–Ω', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω']
        
        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return '–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π'
        elif neg_count > pos_count:
            return '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        else:
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π'
    
    def extract_topic(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º—É —Ç–µ–∫—Å—Ç–∞"""
        topics = {
            '–∞–∫—Ü–∏–∏': ['—Å–∫–∏–¥–∫', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂', '–∞–∫—Ü–∏', '–±–æ–Ω—É—Å', '–ø—Ä–æ–º–æ–∫–æ–¥', '%'],
            '–Ω–æ–≤–æ—Å—Ç–∏': ['–∑–∞–ø—É—Å–∫', '–æ–±–Ω–æ–≤–ª–µ–Ω', '–Ω–æ–≤—ã–π', '–∞–Ω–æ–Ω—Å', '—Ä–µ–ª–∏–∑', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º'],
            '–ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–æ': ['–ø–∞—Ä—Ç–Ω–µ—Ä', '—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', '–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ'],
            '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫', '–æ—Ç–ø—Ä–∞–≤–∫', '–ø–æ–ª—É—á–µ–Ω', '–∫—É—Ä—å–µ—Ä', '–ø—É–Ω–∫—Ç –≤—ã–¥–∞—á–∏'],
            '–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å': ['ozon', 'wildberries', '—è–Ω–¥–µ–∫—Å –º–∞—Ä–∫–µ—Ç', 'marketplace']
        }
        
        text_lower = text.lower()
        for topic, keywords in topics.items():
            if any(keyword in text_lower for keyword in keywords):
                return topic
        
        return '–Ω–æ–≤–æ—Å—Ç–∏'
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'[@#]\w+', '', text)
        text = re.sub(r'[^\w\s\.\!\?,:;-]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def structure_content(self, main_posts: List[str], discussion_posts: List[str]) -> Dict:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é AI"""
        logger.info("üß† AI —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        try:
            main_summaries = []
            main_topics = set()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ—Å—Ç—ã
            for post in main_posts[:3]:
                if len(post.strip()) > 20:
                    summary = self.summarize_text(post)
                    topic = self.extract_topic(post)
                    main_summaries.append(summary)
                    main_topics.add(topic)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏—Å–∫—É—Å—Å–∏–æ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –ø–æ —Ç–µ–º–µ
            discussion_insights = []
            for post in discussion_posts:
                if len(post.strip()) > 20:
                    post_topic = self.extract_topic(post)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã
                    if any(topic in post_topic for topic in main_topics) or not main_topics:
                        sentiment = self.analyze_sentiment(post)
                        summary = self.summarize_text(post)
                        discussion_insights.append({
                            'text': summary,
                            'sentiment': sentiment
                        })
                        if len(discussion_insights) >= 2:  # –ú–∞–∫—Å–∏–º—É–º 2 –∏–Ω—Å–∞–π—Ç–∞
                            break
            
            return {
                'main_topic': ' | '.join(list(main_topics)[:2]) if main_topics else '–Ω–æ–≤–æ—Å—Ç–∏',
                'main_content': main_summaries,
                'discussion_insights': discussion_insights,
                'has_ai_analysis': len(main_summaries) > 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return {
                'main_topic': '–Ω–æ–≤–æ—Å—Ç–∏',
                'main_content': [self.simple_summarize(post) for post in main_posts[:2]],
                'discussion_insights': [],
                'has_ai_analysis': False
            }